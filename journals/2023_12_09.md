- [Excuse me, but the industries AI is disrupting are not lucrative](https://www.theintrinsicperspective.com/p/excuse-me-but-the-industries-ai-is) #ml #economics
	- a thought-provoking bear case on the AI industry _regardless of the technological feasibility of AGI_!
	- > The payment for humans working as “mechanical turks” on Amazon are shockingly low. If a human pretending to be an AI (which is essentially what a mechanical turk worker is doing) only makes a buck an hour, how much will an AI make doing the same thing?
	- > Call it the **supply paradox of AI**: the easier it is to train an AI to do something, the less economically valuable that thing is. After all, the huge supply of *the thing* is how the AI got so good in the first place.
- [The “it” in AI models is the dataset.](https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/) #ml #[[big data]]
	- > What this manifests as is – trained on the same dataset for long enough, pretty much every model with enough weights and training time converges to the same point. Sufficiently large diffusion conv-unets produce the same images as ViT generators. AR sampling produces the same images as diffusion.
	  
	  > This is a surprising observation! It implies that model behavior is not determined by architecture, hyperparameters, or optimizer choices. It’s determined by your dataset, nothing else. Everything else is a means to an end in efficiently delivery compute to approximating that dataset.