- [Exponentially Faster Language Modeling](https://arxiv.org/abs/2311.10770) (Belcak & Wattenhofer 2023) #papers #[[machine learning]]
	- it may be possible to prune which neurons are used during inference to go faster.
	- this uses a "fast feedforward neural network", effectively a trick to put the neurons in a tree structure so irrelevant ones can be ignored
	- this does cause some loss of accuracy, but less than you'd think- benchmarked at 96% of [[GLUE]] performance relative to the model without this optimization