- [Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers](https://arxiv.org/abs/2311.10642) #[[machine learning]] #papers
	- some suggestion that some [[transformer]] models could be simplified to model attention with shallow neural networks trained on the activations of the transformer's attention layer. backs it up with [[BLEU]] scores too!
	- this does require more parameters, though, and reduce model flexibility