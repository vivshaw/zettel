---
tags: books, LLMs, ml, NLP, Sebastian Raschka
---

- # Chapter 1: Understanding LLMs
	- an [[LLM]] is a [[deep neural network]] built to understand, generate, and respond to human language. they're "large" both in the sense of their massive parameter size (billions) and the massive corpora they must be trained on
	- LLMs typically use the [[transformer]] architecture
	- LLMs are useful for all kinds of things- not just generative content, but [[machine translation]], summarization, search...
	- nowadays, [[PyTorch]] is the most common tool for building these sorts of models.
		- (mildly sad to hear that my beloved [[Keras]] ain't in the game)
	- constructing these models is a 2-step process:
		- **[[pretraining]]**, in which we toss a huge amount of raw text at it for some [[self-supervised learning]], so it can model the general structure of language. the result of this stage is a [[foundation model]], which can then be further trained for more specific purposes in the next step.
		- **[[fine-tuning]]**, in which we sculpt the model to do what we want with some [[supervised learning]]. we might train it for [[classification]] by giving it a bunch of labeled examples, or we might do instruction fine tuning with pairs of inputs and desired results (e.g. for translation)
	- you need oodles of data to build one o' these. stuff like [[CommonCrawl]]
	- we're gonna build our own! we'll do it in 3 stages:
		- build the attention mechanism
		- build and pretrain an LLM using that attention mechanism
		- fine-tune a pretrained LLM on a specific task
		- ![image.png](../assets/image_1735968853700_0.png)
- # Chapter 2: Working with text data
	- before we can build an LLM, we need data! that data will be text. lots and lots of text.
	- it can't just be TXT files, though- we need ways to chop it up into tokens, then vectorize those tokens into a representation the LLM can work with
		- in a sense, we're converting the [[categorical]] data that is text into [[numeric]] data
	- an **embedding** maps discrete items, like text, into a continuous vector space. that's the thing we need.
		- these aren't just used at the word or subword level, and not just in prepping text for training! they might be used at the sentence or paragraph level in [[RAG]], for example
	- to do this we need a [[tokenizer]].
		- we can make a simple tokenizer by splitting the text into words and punctuation, taking the set unique items, sorting it, then mapping each item to its index in order. we can then encode and decode into numerical identifiers
	- we will probably want some [[special tokens]], like `<|endoftext|>` for the end of the input, and `<|unk|>` to handle unknown input
		- we might want others, like `<|BOS|>`/`<|EOS|>` for the beginning and end of sequences, or `<|PAD|>` for padding texts out to an equal length.
	- [[byte pair encoding]] is a more sophisticated method
	- to do word prediction, we want a dataset of input-target pairs, which we can produce with a pair of sliding windows spaced one token apart.
	- we'll use an embedding layer to convert tokens into embedding vectors.
		- this works much like [[one-hot encoding]] plus matrix multiplication, but we let the NN optimize it
		- [[PyTorch]]'s `torch.nn.Embedding` can do this
	- but word embeddings isn't enough. sentences are ordered, so we need position information if we're gonna retain all the information.
		- there are two types we could use. an _absolute_ position-aware embedding assigns a unique embedding to each position. a _relative_ position-aware embedding instead assigns embeddings based on the distance between tokens.
- # Chapter 3: Coding attention mechanisms
	- now that we have a dataset, we need the mechanisms that we'll build the neural network on
	- why do we need attention?
		- suppose we're doing [[machine translation]]. it's not possible to do word-for-word translation, because that's not how language works! we need context from elsewhere in the sentence, word order, etc.
		- typically, we use [[encoder-decoder models]]
		- in the past, we'd use [[RNN]]s, which can handle sequential information. the encoder RNN stores that information in a hidden context, then the decoder RNN uses that hidden state as its initial state. but they're very limited! the RNN has only one hidden state at a time, which needs to encapsulate the _entire_ context.
	- attention mechanisms can give us direct access to previous tokens of the input. this was first done by Bahndau et al, who added an attention mechanism to an RNN. that mechanism let the decoder access all the input tokens selectively.
		- but... we discovered that you don't really need the RNN to make this work. just attention is enough. [[transformer]] models use **self-attention**, in which each position of a sequence can determine the relevance of each other position in that sequence.
		- we call it *self*-attention as opposed to, say, [[sequence-to-sequence]] models, where we pay attention to a _different_ sequence
		-