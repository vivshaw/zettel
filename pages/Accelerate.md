---
tags: books, devops, software engineering, engineering management
---

- **Foreward/Intro**
	- this book dispels the myth that there's a tradefoff between speed and safety.
		- it shows a path to get an organization from "deploying once in a while" to "deploying multiple times per day"
		- this doesn't just make the engineers happy- it has real business value! it lets you explore the market, respond to problems, and release new features faster.
		- _and_, it's correlated with quicker recovery from outages.
	- it's not one-size-fits-all, and there are common traps you'll fall into if you try that
		- instead, you need to move to outcome-based team structures. e.g. [[OKR]]s
	- this only works if there's senior leadership support. with actions as well as words
- # Part 1: What We Found
- **Ch. 1: Accelerate**
  collapsed:: true
	- "business as usual" isn't good enough. we need to accelerate delivery of value to the custojmer- in the form of goods and services, market engagement to detect demand, anticipation of regulatory changes, risk response...
	- in the modern world, software is at the core of this
	- the practices developed by the [[devops]] movement are what this book will address and measure
	- **focus on capabilities, not maturity.**
		- > The key to successful change is measuring and understanding the right things with a focus on capabilities [...]
		  (p. 31)
	- why are maturity models bad?
		- maturity models are about arriving at a final state and being "done". but businesses should aim for [[continuous improvement]]
		  logseq.order-list-type:: number
		- maturity models are often a linear, one-size-fits-all formula. that doesn't track the diversity of real-world business.
		  logseq.order-list-type:: number
		- maturity models don't care about outcomes. but we _should_- we should be measuring what matters to us, then figuring out what levers to pull (capabilities) to influence it
		  logseq.order-list-type:: number
		- maturity models judge by a static level of tech progress. but tech is not static!
		  logseq.order-list-type:: number
	- we should use evidence to guide our efforts!
		- technology age, whether ops or dev team deploy, and whether a change approval board exists are all often cited as making a difference, but empirically they don't!
		- this book finds 24 empirically-supported capabilities to build. that's what the rest of the book is about!
- **Ch. 2: Measuring Performance** #metrics
  collapsed:: true
	- flaws in previous attempt to measure performance:
		- they focus on *outputs* (e.g., LOC, PR count, story points) rather than *outcomes*. but outputs prioritize busywork!
		- they focus on *local maxima* over *global maxima*
	- measuring utilization is bad! low utilization is a problem, but 100% utilization is _also_ a problem- you lose all slack capacity for unplanned work, changes, and improvement
	- a package of four metrics for **global outcomes**- two tempo measures, and two stability measures:
		- **delivery lead time:** how long does it take to go from code committed, to code live in prod? alternatively, the time it takes for the build -> test -> deployment flow. (a proxy for lead time in general, but measuring the earlier phases of design & development is too squishy and vague)
		- **deployment frequency:** how often do you deploy? (a proxy for batch size- we want frequent, small changes, rather than huge, slow changes.)
			- small batches are important- they enable fast feedback via AB testing
		- **time to restore service:** when we have an incident, how long does it take to resolve it?
		- **change failure rate:** what % of changes to production fail?
	- used [[cluster analysis]] over 4 years of survey data. this located consistent clusters of high, medium, and low performers.
		- these measures are demonstrated to be statistically valid
		- the high performers showed both higher tempo _and_ higher stability. so there is no tradeoff!
		- some oddities with medium performers- in 2016 having _worse_ stability than low performers, but not other years. hypothesized due to these orgs being midway through rearchitecting
	- ok, this measures engineering performance- but does it have an impact on business performance?
		- sure does- high-performing cluster was twice as likely to exceed ROI targets as low performers
		- also overperformed on non-financial measures like quantity targets, customer satisfaction, etc
	- this has impacts on outsourcing/build-vs-buy decisions. it's a bad call to outsource business-critical software- you need to be bringing delivery capabilities into your org's core priorities. conversely, it's a bad call to in-house non-strategic software like payroll.
	- critical caveat: these measures need to be considered in tandem with company culture. toxic cultures lead to the use of measurements as a tool of control, and in turn reaction against that leads to bad measurements. so, you need to develop a healthy eng culture first!
- **Ch. 3: Measuring and Changing Culture** #culture
  collapsed:: true
	- "culture is king" - but how do we _measure_ something as squishy and abstract as culture?
	- org culture exists at three levels:
		- **basic assumptions** - the things we "just know", formed implicitly over time by working together and making sense of what we experience
		- **values** - visible, explicit collective values and norms of the org
		- **artifacts** - mission statements, technology, procedures, rituals, etc
	- a model defined by [[Ron Westrum]], operating at the values level, and supplying a typology of cultures:
		- **pathological** - power-oriented organizations, characterized by fear and threat. folsk hoard or withhold info for power or propaganda
		- **bureaucratic** - rule-oriented organizations, characterized by departments "protecting turf", working "by the book", red tape
		- **generative** - performance-oriented organizations, which focus on the mission. everything is subordinated to how we effectively accomplish our goals
	- Westrum's characteristics of good information
		- it provides answers to the question the receiver needs answered
		- it's timely
		- it is presented effectively
	- survey used a [[Likert scale]] to assess statements generated from Westrum's model
	- how'd the stats do? great! the model was found to have [[discriminant validity]], [[convergent validity]], and [[reliability (statistics)]], and found that culture impacted both delivery performance and org performance
	- how does generative culture enable information processing?
		- in generative orgs, folks collaborate more effectively and with more trust
		- in generative orgs, the mission is primary
		- in generative orgs, the playing field is more level
	- Google's research into high-performing teams- they thought they'd find a set of individual traits and skills that make you a high performer, but instead they found that who's on a team matters less than how they work and are structured
	- how do orgs deal with accidents? pathological orgs look for "a throat to choke" - finding and blaming the responsible person
		- so, "human error" is not good enough in [[incident response]]! human error tshould be the _start_ of an investigation, not the end. our goal should be finding ways to make human error less likely or less costly, not to point blame
	- how to change culture?
		- first change what people *do*. changing how they *think* comes later.
		- hypothesis: implementing a particular set of practices (agile/lean management, and continuous delivery) impacts culture
		- in fact, they do! survey found that both sets of practices impact culture
- **Ch. 4: Technical Practices**
  collapsed:: true
	- starts with the birth of [[agile]], [[extreme programming]], and [[continuous integration]]/[[continuous delivery]]
	- many organizations treat the _technical_ practices of Agile as less important than the organizational and social practices. this research suggests they're actually quite important.
	- continuous delivery exists to get features to users **safely**, **quickly**, and **sustainably**
	- 5 key principles of continuous delivery:
		- **build quality in**. instead of manually inspecting outcomes, build tools and practices that help you get it right by default, and detect issues that slip through quickly.
		- **work in small batches**. this lets us try things quickly and in small chunks, and course-correct if it doesn't work it. to make this work, it is key to reduce the cost of shipping changes.
		- **computers perform repetitive tasks, people solve problems**. problem-solving work is what's high-value, so we should ensure people can spend as much time as possible focused on it.
		- **relentlessly pursue continuous improvement**. high performance isn't static- you need a feedback loop of improvement
		- **everyone is responsible**. great technical delivery is a system-level outcome, not an individual-level outcome. so we need everyone in the system on-board, and we need ways to make the state of the system visible to them.
	- foundations of continuous delivery:
		- **comprehensive config management**: you need to be able to provision environments, build, test, and deploy in an automated, version-controlled way. use [[IaC]]/[[CaC]] tools, not clickops
		- **continuous integration**: keep branches short-lived, integrate them regularly into `main`, and build & test each change automatically.
		- **continuous testing**: tests shouldn't just be a thing that happens once a feature is release-ready. it should be an integral part of the dev workflow. no work is done until all automated tests pass!
	- Forsgren used [[Likert scale]] surveys once again for empirical research into the impact of CD tools and practices
	- two additional constructs measured:
		- [[loosely-coupled]], well-[[encapsulated]] architecture
		- letting teams choose their own tools
	- empirical impacts of CD:
		- the technical practices discussed here did indeed correlate with on-demand deploys and fast feedback
		- CD drove less rework, Westrum generative org culture and software delivery performance (which in turn drive organizational performance)
		- CD _also_ drove less burnout and less deployment pain
	- how to measure [[quality]]?
		- change fail rates
		- proxy variables:
			- perception of quality and performance of the app
			- percentage of time spent on rework & unplanned work. this is "the difference between paying attention to the low fuel warning light on an automobile versus running out of gas on the highway".
			- percentage of time spent fixing bugs identified by customers
	- what capabilities should you develop for CD?
		- **version control** - you should be using VC, both for code and for config
		- **test automation** - tests should be automated, _reliable_, and primarily written and maintained by the developers who are doing the feature work. these tests should run on every commit
		- **test data management** - you should have adequate data to run a full test suite of your product's features
		- **trunk-based development** - don't use long-lived feature branches! merge into trunk continuously. don't use code freezes or stabilization periods either.
		- **information security** - integrate your infosec practices into your software delivery
- **Ch. 5: Architecture** #[[software architecture]]
  collapsed:: true
	- [[software architecture]] can be a barrier to good delivery. plus, can agile practices work in a mainframe world, hardware world, or massive enterprise?
	- looked at many types of system:
		- **greenfield**: new, unreleased systems
		- **systems of engagement**: used directly by end users
		- **systems of record**: used for data storage
		- **outsourced custom software**
		- **in-house custom software**
		- **off-the-shelf commercial software**
		- **embedded software**
		- **user-installed software**, such as mobile apps
		- **mainframe software**
		- **self-hosted software**
		- **cloud-hosted software**
	- lower performers were more likely to be outsourcing, and more likely to be working on mainframes
	- in all other cases, there was no significant correlation!
	- what's important is architectural _characteristics_, not the specific architecture type:
		- "testability": ability to perform tests without requiring an integrated environment
		- "deployability": ability to deploy independently of the other services it depends on
		- loosely-coupled, well-encapsulated architecture
	- how do we define loose coupling and good encapsulation?
		- ability to make large-scale changes to their system without permission from outside their team
		- ability to make large-scale change to their system without depending on other teams to change _their_ systems
		- ability to complete work without coordinating outside their team
		- ability to deploy and release on command
		- ability to do most testing on command
		- ability to deploy with no downtime during regular business hours
	- communication bandwidth and systems architecture are tightly linked! see [[Conway's law]]
		- this means cross-functional teams are crucial
		- this also means organizations may need to perform a "reverse Conway maneuver", and fix their org structure in order to fix their delivery practices
	- you can apply the principles of loose coupling and good encapsulation to any technical system. it is not synonymous with [[microservices]]- indeed, it's possible to build a microservice-based architecture that fails all of these capabilities!
	- impacts of loosely-coupled architecture:
		- enables scaling by increasing productivity, allowing linear or superlinear productivity growth while growing the team, contra [[The Mythical Man-Month]].
		- enables better delivery tempo and stability while reducing burnout
	- many organizations standardize tools, for a few reasons:
		- reducing environment complexity
		- ensuring the necessary maintenance skills are there in the org
		- increasing vendor purchasing power
		- ensuring correct licensing
	- *however*, letting teams pick their own tools has benefits that outweigh those! they know their goals and understand the problems they're tackling. let them choose the tools that most effectively serve them
	- there is a place for standardization though, in infrastructure. it's useful to have a standard platform.
	- **"architects should focus on engineers and outcomes, not tools or technologies"**
		- what tools you pick is irrelevant if folks hate using them, or if they don't lead to the outcomes you care about!
		- people building the platform need to collaborate closely with users of the platform to get this right.
- **Ch. 6: Infosec** #security
  collapsed:: true
	- infosec teams tend to be a) understaffed, and b) only involved at the end of the software lifecycle. so despite the importance of security, it gets treated as an afterthought!
	- many non-security engineers don't know stuff like the [[OWASP Top 10]]
	- book recommends [[shifting left]] on security- integrating it into the dev cycle earlier, as part of the delivery process. what does that mean?
		- security reviews for all major features, performed in a way that doesn't slow down the dev process
		- infosec integrated into the whole lifecycle, from dev through ops
		- it is easy for devs to do the right thing for security- good automated tools in place, packages available for security needs, processes that help you fall into the pit of success
	- overall, this is a shift from "having security teams do reviews" to "having security teams give devs the means to build in security". this has a few benefits:
		- it's easier for security teams to make sure that folks are doing the right things in-process than to inspect complete, fully-built systems.
		- security teams don't have the capacity to review as frequently as we wish to deploy, so this reduces the bottleneck.
	- devops high performers from the survey spent ~50% less time on security issue remediation
	- names for this approach:
		- [[DevSecOps]]
		- [[Rugged DevOps]]
	- to make it work, everyone needs to be held responsible for security
- **Ch. 7: Management** #[[engineering management]]
  collapsed:: true
	- originally, the [[project management]] and [[program management]] approaches of folks like [[PMI]] and [[PRINCE]] were king
	- later, [[agile]] gained traction
	- at the same time, [[lean manufacturing]] ideas started getting applied to software engineering- see Poppendieck & Poppendieck's [[Lean Software Development]]
	- principles of Lean, as addressed by this survey:
		- limit work-in-progress
		- "visual management" - create visual displays showing the core productivity and quality metrics #dashboard
			- e.g., [[kanban]]
		- "feedback from production" - use data from performance and infrastructure monitoring tools to make business decisions
		- lightweight change approvals
	- *only* when these tools are combined do we see a significant positive effect on software delivery performance. just limiting WIP won't cut it!
		- these tools _also_ lead to more generative culture, and decrease burnout!
	- approval by an external body _actively worsens_ stability! so not only does it slow you down, it defeats its own purpose!
	- the book's recommendation is a peer-review-based change approval, like [[pair programming]] or within-team [[code review]]
		- this should be combined with a [[continuous integration]] pipeline to catch bad changes, so that not everything has to be manual
		- this should be used not just for code, but infrastructure and database changes!
	-
- **Ch. 8: Product Development** #product
  collapsed:: true
	- [[agile]] "won", but a lot of it is fake Agile- organizations borrowing Agile rituals and trapping while continuing to follow a waterfall model and ignoring user research
	- in contrast, [[lean]] product dev involves centering user research from the beginning
	- four capabilities of an org practicing lean product dev:
		- slicing up products and features into small batches, that can be completed in less than a week, and released frequently
		- understanding of and visibility into the flow of work top-to-bottom, all the way out to the customer
		- actively and regularly seeking customer feedback, and incorporating it into the design of the product
		- teams having the authority to create and change specs as part of the dev cycle without requiring outside approval
	- these factors predict software delivery performance, generative culture, and low burnout, and software delivery performance in turn predicts Lean practices. a virtuous cycle!
	- these factors are also interrelated:
		- collecting customer feedback can only be effective if teams have the authority to act on it
		- delivering in small batches is crucial for taking an experimental, feedback-driven approach to product dev
- **Ch. 9: Making Work Sustainable** #burnout
  collapsed:: true
	- we'd like to make sure that software delivery performance doesn't come at the cost of human misery. so let's look at burnout and deployment pain.
	- where we see the worst deployment pain, is also where we see the worst delivery performance, org performance, and culture/ that is to say, the practices that give us speed and stability when delivering, also reduce deployment stress.
	- if deployments have to happen outside business hours, that's a bad sign!
	- what makes deployments miserable?
		- software not designed with deployment in mind. brittle config and poor errors when it fails. (these are also qualities that make it worse for [[distributed systems]]!)
		- need for manual intervention in production in the deploy process, which can lead to typo errors and [[configuration drift]]
		- deployments requiring multiple handoffs between teams, e.g., when devs, DBAs, infosec specialists, and testers work on separate siloed teams
	- so instead, we should:
		- build systems that can be deployed easily, are robust against failures in their environments, and can have components updated independently
		- ensure the state can be reproduced deterministically and automatically via [[IaC]] and [[CaC]]
		- make the deploy process as simple as possible for the deployer
	- **burnout** is "physical, mental, or emotional exhaustion caused by overwork or stress". it often manifests as a feeling of helplessness, or loss of interest. it's correlated with wasteful make-work and pathological cultures.
	- stressful jobs can be as bad for your health as secondhand smoke! it wrecks individuals' lives, as well as teams and organizations
	- managers often try to fix the _person_ and ignore the _work environment_. this cannot work.
	- instead, we should:
		- foster respectful, supportive environments that treat failure as a learning opportunity rather than an opportunity for blame
		- communicate a strong sense of purpose
		- invest in employee development
		- actually _ask_ employees what's stopping them from achieving their goals, then fixing those things
		- give employees time, space, and resources to experiment and learn
		- give employees the authority to make real decisions about their work
	- common causes of burnout:
		- work overload
		- lack of control
		- insufficient reward
		- breakdown of community
		- absence of fairness
		- value conflicts
	- the book's research found that good technical practices and lean practices were correlated with less burnout.
	- from the research, the best factors to nail in order to stop burnout, in order:
		- work culture
		- deployment pain
		- effectiveness of leaders
		- organizational investments in DevOps
		- organizational performance
	- invest in skills and learning! if you don't, you will fail to adopt good practices, and you'll burn people out.
	- support experimentation, failure, and learning! not just that- give engineers time to do experimental, creative work during the work week! if they have to stick around after hours to try new things, they will either not do it, or they'll burn out. examples like Google's 20% time support this
	- behind all this, it's important for the org's values to be aligned with its people. that is to say, the real, de facto, lived values of the org. not the nice words it puts in its values statement. alignment leads to thriving, mismatch leads to burnout.
- **Ch. 10: Employee Satisfaction, Identity, and Engagement**
  collapsed:: true
	- the book's research suggests that engagement and satisfaction help drive good organizational outcomes.
	- Net Promoter Score ([[NPS]]) is used as a measure of employee loyalty, both at the orga nd the team level. they find that high-performing orgs have a 2.2x higher NPS on average.
	- empirical studies have shown that more engaged companies have better revenue growth and higher stock value.
	- NPS was significantly correlated with:
		- the extent to which customer feedback is used
		- the ability to visualize the flow of features through dev all the way out to the customer
		- the extent to which employees identify with their organization's values and goals
	- > [W]hen employees see the connection between the work they do and tits positive impact on customers, they identify more strongly with the company's purpose, which leads to better software delivery and organizational performance.
	- tried to use a series of Likert scales to measure to what extent employees identified with the organization- found that it predicted generative, performance-oriented culture. also that it conflicted with the top-down workflow and planning processes of many orgs!
	- they found that continuous delivery and Lean practices both positively impacted job satisfaction, which in turn positively impacted organizational performance
		- it makes sense that adopting a DevOps approach helps with job satisfaction. humans are bad at and dislike repetitive schleps. so, give those tasks to machines, so we can focus on what we're good at and enjoy!
	- empirical research found that diversity matters too. teams with more diversity tended to have better team performance and better business outcomes.
		- their survey found a much _lower_ rate of women respondents than expected!
- **Ch. 11: Leaders and Managers** #[[engineering management]]